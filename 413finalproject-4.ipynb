{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "import datetime\n",
    "\n",
    "from praw.models import MoreComments\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sys\n",
    "from os import path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from collections import Counter\n",
    "import re, string\n",
    "\n",
    "\n",
    "\n",
    "####for kmeans####\n",
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "####for afn prop####\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn.cluster\n",
    "import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "#### Our goal is to acquire available Reddit data from two internal sources (/r/news and /r/worldnews) to see how they compare and conduct analyzations on.\n",
    "\n",
    "\n",
    "#### We start by pulling the data from the Reddit API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client_id = open(\"client_id\").read()\n",
    "client_secret = open(\"client_secret\").read()\n",
    "password = open(\"password\").read()\n",
    "user_agent = open(\"user_agent\").read()\n",
    "username = open(\"username\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id=client_id,\n",
    "                     client_secret=client_secret,\n",
    "                     user_agent=user_agent,\n",
    "                     password=password,\n",
    "                     username=username)\n",
    "\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getNews(subreddit, n):\n",
    "    results = list()\n",
    "    dic = dict()\n",
    "    # commentDic=dict()\n",
    "    for submission in reddit.subreddit(subreddit).top(limit=n):\n",
    "        tempList = list()\n",
    "        tempList.append(submission.fullname)\n",
    "        tempList.append(re.sub('[%s]' % re.escape(string.punctuation), '', str.lower(submission.title)))\n",
    "        tempList.append(datetime.datetime.fromtimestamp(submission.created))\n",
    "        tempList.append(submission.score)\n",
    "        tempList.append(submission.url)\n",
    "        # tempList.append(submission.comments.list())\n",
    "        # commentDicTemp=dict()\n",
    "        # for comment in submission.comments:\n",
    "        #     if isinstance(comment, MoreComments):\n",
    "        #         continue\n",
    "        #     else:\n",
    "        #         commentDicTemp[comment.id]=comment.body\n",
    "        #         commentDic[comment.id]=comment.body\n",
    "        # tempList.append(commentDicTemp)\n",
    "        dic[submission.id] = tempList\n",
    "    results.append(dic)\n",
    "    # results.append(commentDic)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "startTime = datetime.datetime.now()\n",
    "\n",
    "resultsUS = getNews(\"news\", 999)\n",
    "usNews = resultsUS[0]\n",
    "# usComments = resultsUS[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resultsWorld = getNews(\"worldnews\", 999)\n",
    "worldNews = resultsWorld[0]\n",
    "# worldComments = resultsWorld[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "endTime = datetime.datetime.now()\n",
    "\n",
    "print(startTime)\n",
    "print(endTime)\n",
    "# 10 records ~80 seconds\n",
    "# 100 records ~800 seconds\n",
    "print((endTime - startTime).seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Sentiment Analysis & Descriptive Stats\n",
    "\n",
    "usSent = list()\n",
    "worldSent = list()\n",
    "\n",
    "for k in usNews:\n",
    "    # print(k)\n",
    "    title = usNews[k][1]\n",
    "    n = TextBlob(title)\n",
    "    # print(n)\n",
    "    # print(n.sentiment.polarity)\n",
    "    usSent.append(n.sentiment.polarity)\n",
    "\n",
    "for k in worldNews:\n",
    "    # print(k)\n",
    "    title = worldNews[k][1]\n",
    "    n = TextBlob(title)\n",
    "    # print(n)\n",
    "    # print(n.sentiment.polarity)\n",
    "    worldSent.append(n.sentiment.polarity)\n",
    "\n",
    "usSentAvg = sum(usSent) / float(len(usSent))\n",
    "worldSentAvg = sum(worldSent) / float(len(worldSent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "UsSubScore = list()\n",
    "for i in usNews:\n",
    "    n = (usNews[i][3])\n",
    "    UsSubScore.append(n)\n",
    "\n",
    "WorldSubScore = list()\n",
    "for i in worldNews:\n",
    "    n = (worldNews[i][3])\n",
    "    WorldSubScore.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(\"US\")\n",
    "# print(usSentAvg)\n",
    "# print(min(usSent))\n",
    "# print(max(usSent))\n",
    "#\n",
    "# print(\"World\")\n",
    "# print(worldSentAvg)\n",
    "# print(min(worldSent))\n",
    "# print(max(worldSent))\n",
    "#\n",
    "print(len(UsSubScore))\n",
    "print(len(WorldSubScore))\n",
    "print(len(usSent))\n",
    "print(len(worldSent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "News = pd.DataFrame(\n",
    "    {'World News': worldSent,\n",
    "     'US News': usSent,\n",
    "     'World Post Score': WorldSubScore,\n",
    "     'US Post Score': UsSubScore\n",
    "     })\n",
    "\n",
    "# Descriptive Statistics\n",
    "print(News.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 2: LSA\n",
    "#here i put in our custom stopwords to not have but it dont work\n",
    "custom_stopword=[\"says\",\"said\",\"say\"]\n",
    "stopwords_custom = set(STOPWORDS)\n",
    "stopwords_custom.add(\"said\")\n",
    "stopwords_custom.add(\"says\")\n",
    "stopwords_custom.add(\"say\")\n",
    "\n",
    "def LSA(title_list):\n",
    "    words = []\n",
    "    for i in title_list:\n",
    "        words += i.split()\n",
    "        #below code is edited and not working. i added and custom_stopword and the parenthesis after not in\n",
    "    filtered_words = [word for word in words if word not in stopwords_custom]\n",
    "    terms = {}\n",
    "    for j in filtered_words:\n",
    "        if j not in terms:\n",
    "            terms[j] = 1\n",
    "        else:\n",
    "            terms[j] += 1\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "usNews_titles = []\n",
    "for k in usNews:\n",
    "    usNews_titles.append(usNews[k][1])\n",
    "usLSA = LSA(usNews_titles)\n",
    "print(usLSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "worldNews_titles = []\n",
    "for k in worldNews:\n",
    "    worldNews_titles.append(worldNews[k][1])\n",
    "worldLSA = LSA(worldNews_titles)\n",
    "print(worldLSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 3: By Time Analysis\n",
    "#print(usNews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first do with us, then do with world\n",
    "before_trump = list()\n",
    "after_trump = list()\n",
    "before_trump_world=list()\n",
    "after_trump_world = list()\n",
    "# nov 8 2016\n",
    "winning_date = datetime.datetime.strptime('20161108', \"%Y%m%d\")\n",
    "# cutoff at may 20\n",
    "todays_date = datetime.datetime.strptime('20170520', \"%Y%m%d\")\n",
    "# 194 days difference between winning date and may20 = 20160428\n",
    "start_date = datetime.datetime.strptime('20160428', \"%Y%m%d\")\n",
    "\n",
    "print(winning_date)\n",
    "print(todays_date)\n",
    "print(start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for news_key in usNews:\n",
    "    timeStamp = (usNews[news_key][2])\n",
    "    if timeStamp >= winning_date and timeStamp <= todays_date:\n",
    "        # append to after trump\n",
    "        after_trump.append(usNews[news_key])\n",
    "    elif timeStamp >= start_date and timeStamp < winning_date:\n",
    "        # append to before trump\n",
    "        before_trump.append(usNews[news_key])\n",
    "        # else do nothing, out of sample range\n",
    "for news_key in worldNews:\n",
    "    timeStamp = (worldNews[news_key][2])\n",
    "    if timeStamp >= winning_date and timeStamp <= todays_date:\n",
    "        # append to after trump\n",
    "        after_trump_world.append(worldNews[news_key])\n",
    "    elif timeStamp >= start_date and timeStamp < winning_date:\n",
    "        # append to before trump\n",
    "        before_trump_world.append(worldNews[news_key])\n",
    "        # else do nothing, out of sample range\n",
    "\n",
    "\n",
    "\n",
    "print(len(after_trump))\n",
    "print(len(before_trump))\n",
    "print(len(after_trump_world))\n",
    "print(len(before_trump_world))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# just the titles\n",
    "after_trump_titles_only = list()\n",
    "for l in after_trump:\n",
    "    after_trump_titles_only.append(l[1])\n",
    "before_trump_titles_only = list()\n",
    "for l in before_trump:\n",
    "    before_trump_titles_only.append(l[1])\n",
    "after_trump_titles_only_world = list()\n",
    "for l in after_trump_world:\n",
    "    after_trump_titles_only_world.append(l[1])\n",
    "before_trump_titles_only_world = list()\n",
    "for l in before_trump_world:\n",
    "    before_trump_titles_only_world.append(l[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ATusSent = list()\n",
    "\n",
    "for k in after_trump_titles_only:\n",
    "    title = k\n",
    "    n = TextBlob(title)\n",
    "    ATusSent.append(n.sentiment.polarity)\n",
    "\n",
    "score = list()\n",
    "for k in after_trump:\n",
    "    n = k[3]\n",
    "    score.append(n)\n",
    "\n",
    "AT = pd.DataFrame({\n",
    "    \"After Trump News Sentiment\": ATusSent,\n",
    "    \"After Trump Score\": score\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BTusSent=list()\n",
    "for k in before_trump_titles_only:\n",
    "    title = k\n",
    "    n = TextBlob(title)\n",
    "    BTusSent.append(n.sentiment.polarity)\n",
    "\n",
    "BTscore = list()\n",
    "for k in before_trump:\n",
    "    n = k[3]\n",
    "    BTscore.append(n)\n",
    "\n",
    "BT = pd.DataFrame({\n",
    "    \"Before Trump US News Sentiment\": BTusSent,\n",
    "    \"After Trump Reddit Score\": BTscore\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ATusSent_world = list()\n",
    "\n",
    "for k in after_trump_titles_only_world:\n",
    "    title = k\n",
    "    n = TextBlob(title)\n",
    "    ATusSent_world.append(n.sentiment.polarity)\n",
    "\n",
    "ATworldscore = list()\n",
    "for k in after_trump_world:\n",
    "    n = k[3]\n",
    "    ATworldscore.append(n)\n",
    "\n",
    "AT_world = pd.DataFrame({\n",
    "    \"After Trump News Sentiment\": ATusSent_world,\n",
    "    \"After Trump Score\": ATworldscore\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BTusSent_world=list()\n",
    "for k in before_trump_titles_only_world:\n",
    "    title = k\n",
    "    n = TextBlob(title)\n",
    "    BTusSent_world.append(n.sentiment.polarity)\n",
    "\n",
    "BTscore_world = list()\n",
    "for k in before_trump_world:\n",
    "    n = k[3]\n",
    "    BTscore_world.append(n)\n",
    "\n",
    "BT_world = pd.DataFrame({\n",
    "    \"Before Trump World News Sentiment\": BTusSent_world,\n",
    "    \"After Trump Reddit Score\": BTscore_world\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Before and After Trump Descriptive Statistics on Sentiment for US News\n",
    "print(AT.describe())\n",
    "print(BT.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Before and After Trump Descriptive Statistics on Sentiment for World News\n",
    "print(AT_world.describe())\n",
    "print(BT_world.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "afterTrumpLSA = LSA(after_trump_titles_only)\n",
    "print(afterTrumpLSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "beforeTrumpLSA = LSA(before_trump_titles_only)\n",
    "print(beforeTrumpLSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beforeTrumpLSA_world = LSA(before_trump_titles_only_world)\n",
    "print(beforeTrumpLSA_world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "afterTrumpLSA_world = LSA(after_trump_titles_only_world)\n",
    "print(afterTrumpLSA_world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 4: Visualization - Barplots and Word Clouds\n",
    "#####barplot########\n",
    "\n",
    "def getTopN(d, n):\n",
    "    temp = dict(Counter(d).most_common(n))\n",
    "    keylist = list()\n",
    "    freqlist = list()\n",
    "    for k in temp:\n",
    "        keylist.append(str.upper(k))\n",
    "        freqlist.append(temp[k])\n",
    "    return keylist, freqlist\n",
    "\n",
    "\n",
    "def barplot(names, freqs, title):\n",
    "    x_pos = np.arange(len(names))\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.bar(x_pos, freqs, color='#FFC222', align='center', alpha=0.5, width=.5)\n",
    "    plt.xticks(x_pos, names)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "usNames, usFreq = getTopN(usLSA,15)\n",
    "barplot(usNames,usFreq,\"US Top 15 Title Frequencies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "worldNames, worldFreq = getTopN(worldLSA,15)\n",
    "barplot(worldNames,worldFreq,\"World Top 15 Title Frequencies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beforeTrumpNames, beforeTrumpFreq = getTopN(beforeTrumpLSA,15)\n",
    "barplot(beforeTrumpNames,beforeTrumpFreq,\"Before Trump (US) Top 15 Title Frequencies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "afterTrumpNames, afterTrumpFreq = getTopN(afterTrumpLSA,15)\n",
    "barplot(afterTrumpNames,afterTrumpFreq,\"After Trump (US) Top 15 Title Frequencies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beforeTrumpNamesWorld, beforeTrumpFreqWorld = getTopN(beforeTrumpLSA_world,15)\n",
    "barplot(beforeTrumpNamesWorld,beforeTrumpFreqWorld,\"Before Trump (World) Top 15 Title Frequencies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "afterTrumpNamesWorld, afterTrumpFreqWorld = getTopN(afterTrumpLSA_world,15)\n",
    "barplot(afterTrumpNamesWorld,afterTrumpFreqWorld,\"After Trump (World) Top 15 Title Frequencies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# build stopwords\n",
    "stopwords = set(STOPWORDS)\n",
    "#if we need to add custom stop words we can add them here after wordcloud appears\n",
    "#so we can add any stop words that we may see have too high of frequencies here. code below:\n",
    "#stopwords.add(\"said\")\n",
    "stopwords.add(\"said\")\n",
    "stopwords.add(\"says\")\n",
    "stopwords.add(\"say\")\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "#########wordcloud#########\n",
    "# input a txt file of our words here\n",
    "\n",
    "# takes list of strings\n",
    "# outputs wordclouds\n",
    "def buildWordcloud(t):\n",
    "    samp = t\n",
    "    text = \" \".join(samp)\n",
    "    # use the reddit picture i send you here, only png works i think\n",
    "    reddit_alien = np.array(Image.open(\"redditpic.png\"))\n",
    "    #change the max words and the font size to see what looks best\n",
    "    wc = WordCloud(width=1200, height=1100, background_color=\"white\", max_words=500, mask=reddit_alien,\n",
    "                   stopwords=stopwords, max_font_size=130, random_state=1000)\n",
    "    # generate word cloud\n",
    "    wc.generate(text)\n",
    "    # create coloring from image\n",
    "    image_colors = ImageColorGenerator(reddit_alien)\n",
    "    # show\n",
    "    #makes the picture bigger\n",
    "    plt.figure( figsize=(20,10) )\n",
    "    #gives the wordcloud the same colors as pictures\n",
    "    plt.imshow(wc.recolor(color_func=image_colors), interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "buildWordcloud(usNews_titles)\n",
    "buildWordcloud(worldNews_titles)\n",
    "#\n",
    "buildWordcloud(before_trump_titles_only)\n",
    "buildWordcloud(after_trump_titles_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from our initial barplots we can see that words: say, said, and says are not included in our\n",
    "#stopwords so we are going to have to manually input these in order to get a better visualiztion\n",
    "#and a more accurate analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#makes our list of words all capitals so it looks better in word cloud\n",
    "us_upper = [x.upper() for x in usNews_titles]\n",
    "world_upper= [x.upper() for x in worldNews_titles]\n",
    "bt_upper= [x.upper() for x in before_trump_titles_only]\n",
    "at_upper= [x.upper() for x in after_trump_titles_only]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "buildWordcloud(us_upper)\n",
    "buildWordcloud(world_upper)\n",
    "buildWordcloud(bt_upper)\n",
    "buildWordcloud(at_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 5 (Bonus): Comment stuff AND/OR title classifications unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#kmeans setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "# parse commandline arguments\n",
    "op = OptionParser()\n",
    "op.add_option(\"--lsa\",\n",
    "              dest=\"n_components\", type=\"int\",\n",
    "              help=\"Preprocess documents with latent semantic analysis.\")\n",
    "op.add_option(\"--no-minibatch\",\n",
    "              action=\"store_false\", dest=\"minibatch\", default=True,\n",
    "              help=\"Use ordinary k-means algorithm (in batch mode).\")\n",
    "op.add_option(\"--no-idf\",\n",
    "              action=\"store_false\", dest=\"use_idf\", default=True,\n",
    "              help=\"Disable Inverse Document Frequency feature weighting.\")\n",
    "op.add_option(\"--use-hashing\",\n",
    "              action=\"store_true\", default=False,\n",
    "              help=\"Use a hashing feature vectorizer\")\n",
    "op.add_option(\"--n-features\", type=int, default=10000,\n",
    "              help=\"Maximum number of features (dimensions)\"\n",
    "                   \" to extract from text.\")\n",
    "op.add_option(\"--verbose\",\n",
    "              action=\"store_true\", dest=\"verbose\", default=False,\n",
    "              help=\"Print progress reports inside k-means algorithm.\")\n",
    "\n",
    "print(__doc__)\n",
    "op.print_help()\n",
    "\n",
    "(opts, args) = op.parse_args([\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kmeans(data_set, k, data_name):\n",
    "    print()\n",
    "    print(\"Kmeans for k = \" + str(k) + \" on \" + data_name)\n",
    "    print()\n",
    "    categories = None\n",
    "    dataset = data_set\n",
    "    true_k = k\n",
    "\n",
    "    if opts.use_hashing:\n",
    "        if opts.use_idf:\n",
    "            # Perform an IDF normalization on the output of HashingVectorizer\n",
    "            hasher = HashingVectorizer(n_features=opts.n_features,\n",
    "                                       stop_words=stopwords, non_negative=True,\n",
    "                                       norm=None, binary=False)\n",
    "            vectorizer = make_pipeline(hasher, TfidfTransformer())\n",
    "        else:\n",
    "            vectorizer = HashingVectorizer(n_features=opts.n_features,\n",
    "                                           stop_words=stopwords,\n",
    "                                           non_negative=False, norm='l2',\n",
    "                                           binary=False)\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features,\n",
    "                                     min_df=2, stop_words=stopwords,\n",
    "                                     use_idf=opts.use_idf)\n",
    "    X = vectorizer.fit_transform(dataset)\n",
    "\n",
    "\n",
    "    if opts.n_components:\n",
    "        print(\"Performing dimensionality reduction using LSA\")\n",
    "        t0 = time()\n",
    "        # Vectorizer results are normalized, which makes KMeans behave as\n",
    "        # spherical k-means for better results. Since LSA/SVD results are\n",
    "        # not normalized, we have to redo the normalization.\n",
    "        svd = TruncatedSVD(opts.n_components)\n",
    "        normalizer = Normalizer(copy=False)\n",
    "        lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "        X = lsa.fit_transform(X)\n",
    "\n",
    "        explained_variance = svd.explained_variance_ratio_.sum()\n",
    "        print(\"Explained variance of the SVD step: {}%\".format(\n",
    "            int(explained_variance * 100)))\n",
    "\n",
    "        print()\n",
    "\n",
    "    if opts.minibatch:\n",
    "        km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,\n",
    "                             init_size=1000, batch_size=1000, verbose=opts.verbose)\n",
    "    else:\n",
    "        km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,\n",
    "                    verbose=opts.verbose)\n",
    "\n",
    "    print(\"Clustering sparse data with %s\" % km)\n",
    "    km.fit(X)\n",
    "    if not opts.use_hashing:\n",
    "        print(\"Top terms per cluster:\")\n",
    "\n",
    "        if opts.n_components:\n",
    "            original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "            order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "        else:\n",
    "            order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "        terms = vectorizer.get_feature_names()\n",
    "        for i in range(true_k):\n",
    "            print(\"Cluster %d:\" % i, end='')\n",
    "            for ind in order_centroids[i, :10]:\n",
    "                print(' %s' % terms[ind], end='')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans(usNews_titles,10, \"US News Titles\")\n",
    "kmeans(worldNews_titles,10, \"World News Titles\")\n",
    "\n",
    "kmeans(usNews_titles,5, \"US News Titles\")\n",
    "kmeans(worldNews_titles,5, \"World News Titles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make list of titles into a string and filter out stopwords\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import sklearn.cluster\n",
    "import distance\n",
    "\n",
    "#takes news list, n size of sample, d dampening (.5 < d < 1)\n",
    "def affinity_propagation(news_list,n,d):\n",
    "    np.random.seed(1)\n",
    "    News_string = ' '.join(news_list)\n",
    "    #sampleUS = usNews_string[0:2000]\n",
    "    #sampleUS[0:200]\n",
    "    News_string_filtered = ' '.join([word for word in News_string.split() if word not in (stopwords.words('english') or custom_stopwords)])\n",
    "    #with sample of 10,000 is fine. doesnt take too long yet\n",
    "    sample = News_string_filtered[0:n]\n",
    "    #below words = to what we want clustered. so need to use this for world too. trump?\n",
    "    words = sample.split(\" \") #Replace this line\n",
    "    words = np.asarray(words) #So that indexing with a list will work\n",
    "    lev_similarity = -1*np.array([[distance.levenshtein(w1,w2) for w1 in words] for w2 in words])\n",
    "\n",
    "    affprop = sklearn.cluster.AffinityPropagation(affinity=\"precomputed\", damping=d)\n",
    "    affprop.fit(lev_similarity)\n",
    "    for cluster_id in np.unique(affprop.labels_):\n",
    "        exemplar = words[affprop.cluster_centers_indices_[cluster_id]]\n",
    "        cluster = np.unique(words[np.nonzero(affprop.labels_==cluster_id)])\n",
    "        cluster_str = \", \".join(cluster)\n",
    "        print(\"[%s]: %s\" % (str.upper(exemplar), cluster_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "affinity_propagation(usNews_titles, 1000, .98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
